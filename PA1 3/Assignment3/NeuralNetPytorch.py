# -*- coding: utf-8 -*-
"""Programming Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12IIV_DSxoE7hXnAmAZB9cl3tijpN_HbX
"""

# Commented out IPython magic to ensure Python compatibility.
# %ls

#Mounting the notMNIST dataset that will be used to test and compare CNN and FNN
# from google.colab import drive
# drive.mount('/content/gdrive/', force_remount=True)

# from google.colab import files
# uploaded = files.upload()

# import os
# os.chdir("/content/drive/My Drive/Colab Notebooks")
# !ls


# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt
import numpy as np

# Function for loading notMNIST Dataset
# "/content/drive/MyDrive/Colab Notebooks/notMNIST.npz"
def loadData(datafile = "notMNIST.npz"):
    with np.load(datafile) as data:
        Data, Target = data["images"].astype(np.float32), data["labels"]
        np.random.seed(521)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data = Data[randIndx] / 255.0
        Target = Target[randIndx]
        trainData, trainTarget = Data[:10000], Target[:10000]
        validData, validTarget = Data[10000:16000], Target[10000:16000]
        testData, testTarget = Data[16000:], Target[16000:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

# Custom Dataset class. 
class notMNIST(Dataset):
    def __init__(self, annotations, images, transform=None, target_transform=None):
        self.img_labels = annotations
        self.imgs = images
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        image = self.imgs[idx]
        label = self.img_labels[idx]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label

#Define CNN
class CNN(nn.Module):
    def __init__(self, drop_out_p=0.0):
        super(CNN, self).__init__()
        # Step 2
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=4)
        self.bn1 = nn.BatchNorm2d(num_features=32)
        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)
        #step 3
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4)
        self.bn2 = nn.BatchNorm2d(num_features=64)
        self.pool2 = nn.MaxPool2d(kernel_size=2,stride =2)

        # x = torch.flatten(x, start_dim=1)
        self.dropout = nn.Dropout(p=drop_out_p)

        self.fc1 = nn.Linear(1024, 784)
        self.fc2 = nn.Linear(784, 10)
        #TODO

    def forward(self, x):
        #TODO
        #DEFINE YOUR FORWARD FUNCTION HERE
        x = self.conv1(x)
        x = F.relu(x)
        x = self.bn1(x)
        x = self.pool1(x)

        x = self.conv2(x)
        x = F.relu(x)
        x = self.bn2(x)
        x = self.pool2(x)
        x = torch.flatten(x, start_dim=1)

        x = self.dropout(x)
        x = self.fc1(x)
        x = F.relu(x)
        out = self.fc2(x)
        return out

#Define FNN
# Code
class FNN(nn.Module):
    def __init__(self, drop_out_p=0.0):
        super(FNN, self).__init__()
        # BATCH_SIZE = 32
        #TODO
        #DEFINE YOUR LAYERS HERE
        #  Input layer is BATCH_SIZE x 1 x 28 x 28 Second dim = (1) -> image channel is gray-scale img. 3rd and 4th are WxH
        # transform this matrix into batch of 1D arrays of size: BATCH_SIZE x784
        
        #NOW: FNN followed by Relu activation funch. Size of weight matrix - 784X10 -> 10 is size of 2nd hidden layer
    
        # self.name = "FNN"
      
        self.fc1 = nn.Linear(1*28*28,10) 
        self.fc2 = nn.Linear(10,10)

        self.dropout = nn.Dropout(p=drop_out_p)
        self.fc3 = nn.Linear(10, 10)
        # number of input features

    def forward(self, x):
        #TODO
        #DEFINE YOUR FORWARD FUNCTION HERE
        # STEP 2: A FC layer followed by a Reluactication. The size of the weight matriz is 
        # x = x.view(-1, 53*53*10)
        x = torch.flatten(x, start_dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))

        x = self.dropout(x)
        out = self.fc3(x)
        return out


# Commented out IPython magic to ensure Python compatibility.
# Compute accuracy
def get_accuracy(model, dataloader):
    
    model.eval()
    device = next(model.parameters()).device
    accurateLabel = 0.0
    totalSize = 0.0
    
    with torch.no_grad():
        for data in dataloader:
            images, labels = data
            images = images.to(device)
            labels = labels.to(device) 
            # TODO
            # Return the accuracy
            outputY = model(images)
            hypothesisY = torch.argmax(outputY, dim=1)
            accurateLabel += (hypothesisY == labels).sum().item()
            totalSize += len(labels)
    
    accuracy = (accurateLabel / totalSize) * 100
    return accuracy




def train(model, device, learning_rate, weight_decay, train_loader, val_loader, test_loader, num_epochs=50, verbose=True):
  #TODO
  # Define your cross entropy loss function here 
  # Use cross entropy loss
  model = model.cuda()
  criterion = nn.CrossEntropyLoss()
   #TODO
  # Define your optimizer here
  # Use AdamW optimizer, set the weights, learning rate and weight decay argument.
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=weight_decay)

  acc_hist = {'train':[], 'val':[], 'test': []}

  for epoch in range(num_epochs):
    train_running_loss = 0.0
    train_acc = 0.0
    model = model.train()
    
    ## training step
    for i, (images, labels) in enumerate(train_loader):
        
        images = images.to(device)
        labels = labels.to(device)

        # TODO
        # Follow the step in the tutorial
        ## forward + backprop + loss
        logits = model(images)
        loss = criterion(logits, labels)
        optimizer.zero_grad()
        loss.backward()

        ## update model params
        optimizer.step()        
        # train_running_loss += loss.detach().item()
        # train_acc += get_accuracy(labels, train_loader)
        
    model.eval()
    acc_hist['train'].append(get_accuracy(model, train_loader))
    acc_hist['val'].append(get_accuracy(model, val_loader))
    acc_hist['test'].append(get_accuracy(model, test_loader))
    
    if verbose:
      print('Epoch: %d | Train Accuracy: %.2f | Validation Accuracy: %.2f | Test Accuracy: %.2f' \
          %(epoch, acc_hist['train'][-1], acc_hist['val'][-1], acc_hist['test'][-1]))

  return model, acc_hist

def experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0.5, weight_decay=0.01, num_epochs=50, verbose=False):
  # Use GPU if it is available.
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

  # Inpute Batch size:
  BATCH_SIZE = 32

  # Convert images to tensor
  transform = transforms.Compose(
      [transforms.ToTensor()])

  # Get train, validation and test data loader.
  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()

  train_data = notMNIST(trainTarget, trainData, transform=transform)
  val_data = notMNIST(validTarget, validData, transform=transform)
  test_data = notMNIST(testTarget, testData, transform=transform)


  train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
  val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)
  test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)

  # Specify which model to use
  if model_type == 'CNN':
    model = CNN(dropout_rate)
  elif model_type == 'FNN':
    model = FNN(dropout_rate)

  
  # Loading model into device
  model = model.to(device)
  criterion = nn.CrossEntropyLoss()
  model, acc_hist = train(model, device, learning_rate, weight_decay, train_loader, val_loader, test_loader, num_epochs=num_epochs, verbose=verbose)
  
  # Release the model from the GPU (else the memory wont hold up)
  model.cpu()

  return model, acc_hist


import matplotlib.pyplot as plt

def plotFNN(acc_histFNN):
    fig, axs = plt.subplots(1, 2, figsize=(12, 6))

    axs[0].set_title("FNN Training Accuracy")
    axs[0].plot(range(len(acc_histFNN['train'])), acc_histFNN['train'], linewidth=3.0)
    axs[0].set_xlabel("Epoch")
    axs[0].set_ylabel("Accuracy")
    
    # axs[0].legend()
    axs[1].set_title("FNN Testing Accuracy")
    axs[1].plot(range(len(acc_histFNN['test'])), acc_histFNN['test'], linewidth=3.0)
    axs[1].set_xlabel("Epoch")
    axs[1].set_ylabel("Accuracy")
    

    # axs[0].legend()

    plt.show()

def plotCNN(acc_histCNN):
    fig, axs = plt.subplots(1, 2, figsize=(12, 6))
    
    axs[0].plot(range(len(acc_histCNN['train'])), acc_histCNN['train'], linewidth=3.0)
    axs[0].set_title("CNN Training Accuracy")
    axs[0].set_xlabel("Epoch")
    axs[0].set_ylabel("Accuracy")
    # axs[0].legend()

    axs[1].plot(range(len(acc_histCNN['test'])), acc_histCNN['test'], linewidth=3.0)
    axs[1].set_title("CNN Testing Accuracy")
    axs[1].set_xlabel("Epoch")
    axs[1].set_ylabel("Accuracy")
    # axs[0].legend()

    plt.show()

def compare_arch():
    print("          Experiement 1: FNN AND CNN IMAGE COMPARISON\n")
    FNNM,acc_histFNN = experiment(model_type='FNN',learning_rate= 0.0001,dropout_rate=0.0,weight_decay= 0.00,num_epochs= 50,verbose= False)
    CNNM,acc_histCNN = experiment(model_type='CNN',learning_rate= 0.0001,dropout_rate=0.0,weight_decay= 0.00,num_epochs= 50,verbose= False)
    plotFNN(acc_histFNN)
    plotCNN(acc_histCNN)

compare_arch()

def compare_dropout():
  print("          Experiement 2: DROPOUT RATE EFFECTS ON CNN\n")
  print("DROPOUT PLOTS for RATE = 0.5")
  CNNM,acc_histCNN = experiment(model_type='CNN',dropout_rate=0.5)
  plotCNN(acc_histCNN)
  print("DROPOUT PLOTS for RATE = 0.8")
  CNNM,acc_histCNN = experiment(model_type='CNN',dropout_rate=0.8)
  plotCNN(acc_histCNN)
  print("DROPOUT PLOTS for RATE = 0.95")
  CNNM,acc_histCNN = experiment(model_type='CNN',dropout_rate=0.95)
  plotCNN(acc_histCNN)
compare_dropout()

def compare_l2t():
  print("          Experiement 3: Weight Decay EFFECTS ON CNN\n")
  print("Weight Decay PLOTS for WD = 0.1")
  CNNM,acc_histCNN = experiment(model_type='CNN',weight_decay= 0.1)
  plotCNN(acc_histCNN)
  print("Weight Decay PLOTS for WD = 1.0")
  CNNM,acc_histCNN = experiment(model_type='CNN',weight_decay= 1.0)
  plotCNN(acc_histCNN)
  print("Weight Decay PLOTS for WD = 10.0")
  CNNM,acc_histCNN = experiment(model_type='CNN',weight_decay= 10.0)
  plotCNN(acc_histCNN)
compare_l2t()